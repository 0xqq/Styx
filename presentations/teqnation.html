<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>TeqNation</title>

	<meta name="description" content="Styx Streaming Analytics">
	<meta name="author" content="Bas Geerdink">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/bas.css" id="theme">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

	<!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">

	<!-- Any section element inside of this container is displayed as a slide -->
	<div class="slides">
		<section>
			<h1>Streaming Analytics</h1>
            <h3>How to get fast predictions from real-time data</h3>
            <h3>with Flink, Kafka, and Cassandra</h3>
            <img src="img/TeqnationLogo.png" class="stretch" />
			<p>
				<small>Bas Geerdink | <i>TeqNation</i> | 26-04-2018</small>
			</p>
		</section>

		<section>
			<h2>Agenda</h2>
			<ol>
                <li>Streaming Analytics use cases</li>
                <li>Architecture of Styx</li>
                <li>Technology</li>
                <li>Deep dive:</li>
                <li><ul>
                    <li>Kafka-Flink connection</li>
                    <li>Parallelism</li>
                    <li>Event Time, Windows, and Watermarks</li>
                    <li>CEP</li>
                    <li>State</li>
                    <li>Exactly-once Processing</li>
                    <li>Model scoring</li>
                </ul></li>
                <li>Wrap-up</li>
			</ol>
		</section>

        <section>
            <h2>Sample use Cases</h2>
            <small>
                <!-- Speaker notes: explain customer journeys -->
                <table>
                    <thead><tr>
                        <th>Data source</th>
                        <th>Pattern</th>
                        <th>Prediction</th>
                        <th>Notification</th>
                    </tr></thead>
                    <tbody>
                        <tr>
                            <td>Logs</td>
                            <td>CPU utilization</td>
                            <td>System failure</td>
                            <td>Alert to sys admin</td>
                        </tr>
                        <tr>
                            <td>Tweets</td>
                            <td>Sentiment analysis</td>
                            <td>Too much negativity</td>
                            <td>Social media team</td>
                        </tr>
                        <tr>
                            <td>Debit card depreciation</td>
                            <td>Shopping</td>
                            <td>No more credit</td>
                            <td>Actionable insight</td>
                        </tr>
                        <tr>
                            <td>Payment data</td>
                            <td>Fraud detection</td>
                            <td>Illegal operation</td>
                            <td>Block money transfer</td>
                        </tr>
                        <tr>
                            <td>URL visits / click events</td>
                            <td>Filling in forms</td>
                            <td>Customer is stuck</td>
                            <td>Helpdesk call</td>
                        </tr>
                        <tr>
                            <td>Patient data</td>
                            <td>Rising heart beat</td>
                            <td>Heart failure</td>
                            <td>Alert to doctor</td>
                        </tr>
                        <tr>
                            <td>Traffic</td>
                            <td>Number of cars passing</td>
                            <td>Traffic jam</td>
                            <td>Update route info</td>
                        </tr>
                    </tbody>
                </table>
            </small>
        </section>

        <section>
            <h2>Use cases</h2>
            The common pattern in all these scenarios:
            <ol>
                <li>Detect pattern by combining data (CEP)</li>
                <!-- Digital Signal Processing -->
                <li>Make prediction about the future (ML)</li>
                <!-- Artificial Intelligence -->
                <li>Determine next action</li>
            </ol>
        </section>

		<section>
            <h2>Architecture</h2>
            <img src="img/architecture.png" class="stretch">
        </section>

        <section>
            <h2>Technology</h2>
            <!-- At ING, we created a framework called Styx. It is now open source. We made some tehcnology choices: -->
            Styx uses a selection of source frameworks
            <ul>
                <li>Data stream storage: <b>Kafka</b></li>
                <li>Stream processing: <b>Flink</b></li>
                <li>State management and Cache: <b>Cassandra</b></li>
                <li>Model scoring: <b>PMML</b> and <b>Openscoring.io</b></li>
            </ul>
            <!-- All: scalable, mature, fault-tolerant, fast -->
        </section>

        <section>
            <h2>Deep dive part 1</h2>
            <img src="img/architecture.png" class="stretch">
        </section>

        <section>
            <h2>Set up a Flink application</h2>
            <ul>
                <li>A Flink job is a Java/Scala application</li>
                <li>Jobs can be deployed to a running Flink cluster</li>
                <li>Maven dependencies: </li>
            </ul>
            <pre><code data-trim>
                <dependency>
                  <groupId>org.apache.flink</groupId>
                  <artifactId>flink-scala_2.11</artifactId>
                  <version>1.4.2</version>
                  <scope>provided</scope>
                </dependency>
                <dependency>
                  <groupId>org.apache.flink</groupId>
                  <artifactId>flink-streaming-scala_2.11</artifactId>
                  <version>1.4.2</version>
                  <scope>provided</scope>
                </dependency>
            </code></pre>
        </section>

        <section>
            <h2>Flink-Kafka integration</h2>
            The <i>FlinkKafkaConsumer</i> and <i>FlinkKafkaProducer</i> classes provide easy connectivity
            <!-- Simplified code: serializing data and making a DataStream of 'raw' Kafka events -->
            <pre><code data-trim>
                    def createKafkaEventStream(env: StreamExecutionEnvironment):
                          DataStream[RawEvent] = {

                    // create KeyedDeserializationSchema
                    val readSchema = KafkaSchemaFactory.createKeyedDeserializer
                            (readTopicDef, rawEventFromPayload)

                    // consume events
                    val rawEventSource = new FlinkKafkaConsumer010[BaseKafkaEvent]
                            (toStringList(readTopicDef), readSchema, props)

                    // create source
                    env.addSource(rawEventSource)
                      .filter(_.isSuccess)   // if decrypting from kafka succeeded
                      .flatMap(_.toOption)
                  }
            </code></pre>
        </section>

        <section>
            <h2>Parallelism</h2>
            <ul>
                <li>To get high throughput, we have to process the events in parallel</li>
                <li>The <i>keyBy</i> function distributes our stream into parallel parts</li>
            </ul>
            <pre><code data-trim>
                  def createCepPipeline(sourceStream: DataStream[RawEvent]):
                        DataStream[BusinessEvent] = {
                    val windowStream = sourceStream
                      .keyBy(_.customerId)                // create parallelism
                      .window(GlobalWindows.create())     // window per key
                      .apply(new WindowResultFunction())  // logic: pattern match
                      .map(event => event.addTimeStamp("window"))
                  }
            </code></pre>
        </section>

        <section>
            <h2>Event time and Windows</h2>
            <ul>
                <li>Events occur at certain time <span class="fragment fade-in" data-fragment-index="2">
                    <b>=> event time</b></span></li>
                <li class="fragment" data-fragment-index="1">... but arrive a little while later <span class="fragment fade-in" data-fragment-index="2">
                    <b>=> processing time</b></span></li>

                <!-- a window is a bucket of time -->
                <li class="fragment" data-fragment-index="3">
                    Events can reach us out-of-order; by looking at the event time this doesn't matter</li>
                <li class="fragment" data-fragment-index="4">
                    In processing infinite streams, we usually look at a time <i>window</i>:
                    <ul>
                        <li>Sliding window</li> <!-- e.g. looking back at the last hour -->
                        <li>Tumbling window</li> <!-- e.g. processing data per day -->
                        <li>Session window</li> <!-- based on event data -->
                        <li>Global window</li> <!-- e.g. per key -->
                    </ul>
                </li>
                <!-- TODO: examples -->
                <!-- Windows can be very large (e.g. weeks) and thereby gather a lot of state -->
                <!-- Lesson: always consider event time, make sure that your event data has it -->
            </ul>
        </section>

        <section>
            <h2>Event time and Windows</h2>
            <img>
        </section>

        <section>
            <h2>Event time and Windows</h2>
            <ul>
                <li>We have to tell Flink what data field contains the event timestamps</li>
            </ul>
            <pre><code data-trim>
                TODO
            </code></pre>

            <ul>
                <li>Then, we set the event time characterics of the environment</li>
            </ul>
            <pre><code data-trim>
                env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
            </code></pre>
        </section>

        <section>
            <h2>Watermarks</h2>
            <ul>
                <!-- IF eventTime < watermark THEN Ignore ELSE Process -->
                <li>Watermarks are timestamps that trigger the computation of the window</li>
                <li>By default, any event that reaches the processor later than the watermark, but with an event time that should belong to the previous window, is ignored</li>
                <li>Flink also allows recomputation of the window by increasing the <i>allowedLateness</i></li>
                <!-- the default allowedLateness is 0 -->
                <li>There are <i>periodic</i> (timer-based) and <i>punctuated</i> (stream-based) watermarks</li>
                <!-- example: tumbling window of one hour, watermark is 1 minute. af 14:02 an event comes in with an event time of 13:59 -->
            </ul>
        </section>

        <section>
            <h2>Watermarks</h2>
            <pre><code data-trim>
                def setWatermarks(env: StreamExecutionEnvironment, styxConfig: Config): Unit = {
                  // ...
                  styxConfig.optLong("flink.autowatermark-interval").foreach(
                    autowatermarkInterval => {
                      env.getConfig.setAutoWatermarkInterval(autowatermarkInterval)
                    }
                  )
                }

                // TODO: show event time interval for taxi events?

            </code></pre>
        </section>

        <section>
            <h2>Complex Event Processing</h2>

        </section>

        <section>
            <h2>Complex Event Processing</h2>
            <pre><code data-trim>
            </code></pre>
        </section>

        <section>
            <h2>Savepointing and checkpointing</h2>
            <ul>
                <li>A <i>checkpoint</i> is a periodic dump to file of the in-memory state</li>
                <li>A <i>savepoint</i> is a manual checkpoint</li>
                <li>The state dumps can be used for recovery, replay, or for backup/lineage</li>
            </ul>
            <pre><code data-trim>
#==============================================================================
# Streaming state checkpointing
#==============================================================================

# The backend that will be used to store operator state checkpoints if
# checkpointing is enabled.
#
# Supported backends: jobmanager, filesystem, rocksdb, <class-name-of-factory>
#
state.backend: filesystem
            </code></pre>
        </section>

        <section>
            <h2>Exactly-once processing</h2>
            <ul>

            </ul>
        </section>

        <section>
            <h2>Exactly-once processing</h2>
            <pre><code data-trim>
            </code></pre>
        </section>

        <section>
            <h2>Model scoring</h2>
            <ul>
                <li>After a <i>keyBy</i> operator, we can <i>flatMap</i> over the events</li>
                <li>or apply functions that process the data in the windows</li>
                <li>For example:
                    <ol>
                        <li>enrich each business event by getting more data</li>
                        <li>score a machine learning model on each enriched event</li>
                        <li>write the outcome to a new event / output stream</li>
                    </ol>
                </li>
        </section>

        <section>
            <h2>Model scoring</h2>
            <pre><code data-trim>
            def score(event: RichBusinessEvent, pmmlModel: PmmlModel): Double = {
                val arguments = new util.LinkedHashMap[FieldName, FieldValue]

                for (inputField: InputField <- pmmlModel.getInputFields.asScala) {
                    arguments.put(inputField.getField.getName,
                        inputField.prepare(customer.all(fieldName.getValue)))
                }

                // return the notification with a relevancy score
                val results = pmmlModel.evaluate(arguments)

                pmmlModel.getTargetFields.asScala.headOption match {
                    case Some(targetField) =>
                        val targetFieldValue = results.get(targetField.getName)
                    case _ => throw new Exception("No valid target")
                    }
                }
            }
            </code></pre>

        </section>

        <section>
            <h2>Wrap-up</h2>
            <ul>
                <li>There are plenty of streaming analytics use cases, in any business domain.</li>
                <li>The common pattern is: CEP --> ML --> Notification</li>
                <li>Pick the right tools for the job; Kafka and Flink are amongst the best</li>
                <li>Be aware of typical 'fast data' problems: latency, late events, state management, distribution</li>
                <li>Styx is a proven open source framework for Streaming Analytics that is flexible and scalable</li>
            </ul>
        </section>

        <section>
            <h2>Thanks!</h2>
            <p>Read more about streaming analytics at:</p>
            <ul>
                <li><a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">The world beyond batch: Streaming 101</a></li>
                <li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf">Google Dataflow paper</a></li>
            </ul>
            <p>Source code and presentation is available at:</p>
            <p>
                <a href="https://github.com/streaming-analytics/Styx">https://github.com/streaming-analytics/Styx</a>
            </p>
        </section>

	</div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/search/search.js', async: true },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

</body>
</html>
